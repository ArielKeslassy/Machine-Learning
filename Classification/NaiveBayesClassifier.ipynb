{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Naive Bayes Classifier from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "- Problem type: classification\n",
    "- Assumptions on data: Independence between the features\n",
    "- Theory: Bayes' theorem is mathematical formula used for calculating conditional probabilities.\n",
    "    $ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $\n",
    "    - A and B are events and $P(B)\\neq 0$.\n",
    "    - P(A), P(B) are called the marginal probability, and represents the probability of an event irrespective of the outcomes of other random variables.\n",
    "    - $P(A|B)$ is a conditional probability: the likelihood of event A occurring given that B occurred.\n",
    "    A and B must be different events.\n",
    "    - and for n different classes, and y as a data point that we would like to assign a class:\n",
    " $P(y|x_1, x_2,...,x_n) = \\frac{P(x_1, x_2,...,x_n|y) \\cdot P(y)}{P(x_1, x_2,...,x_n)}$\n",
    "\n",
    "## Naive Bayes:\n",
    "\n",
    "The approach presented above demands a large amount of information, in order to estimate the probability distribution for all different possible combinations of values.\n",
    "\n",
    "Instead I will use the \"Naive Bayes\" approach: I will assume __independency__ between every pair of features (and will preprocces the data accordingly). The independency assumption gives $P(B) \\cdot P(A|B)=P(B) \\cdot P(A)$ , and therefore the calculated probabilty can be simplified to:\n",
    "### $P(y|x) = \\frac{P(y) \\cdot \\prod_{i=1}^{n} P(x_i|y)}{P(x_1, x_2,...,x_n)}$\n",
    "\n",
    "We will need to determine what class gets the highest probability for each data point. Solely for the purpose of comparison, we can calculate just the numerator (because the denominator is constant):\n",
    "### $P(y|x) \\propto P(y) \\cdot \\prod_{i=1}^{n} P(x_i|y)$\n",
    "\n",
    "- note: by calculating just the numerator we lower our precision in __predicting__ the probabilities of a data point being in a class. therefore, we refer mainly to the __comparison__ between classes (by getting the maximum value) rather than the actual probabilty value for the data point being in that class.\n",
    "    \n",
    "Another simplification is needed: calculating the above mentioned equation still requires claculating the conditional probabilities of $P(x_i|y)$. We can avoid doing so using __probability density function__ (PDF).)\n",
    "(https://en.wikipedia.org/wiki/Probability_density_function, https://en.wikipedia.org/wiki/Bayes%27_theorem#:~:text=In%20principle%2C%20Bayes'%20theorem%20applies,relevant%20densities%20(see%20Derivation)\n",
    "    For continuous random variables, the PDF is defined: $P[a \\leq X \\leq b] = \\int_a^b {f(x)dx}$ where f is the PDF.\n",
    "Probabilty for a specific event to occur will be $P(X=x_0) = F(x_0) - F(x_0) = 0$ (Newton-Leibniz formula). Instead of observing specific  (discrete) events, we will observe a small region of events $P(|X-x_0| < Δ(x))$.\n",
    "For this purpose the PDF can be described as $pdf(|X-x_0| < Δ(x))= \\frac {P(|X-x_0| < Δ(x))}{Δ(x)}$.\n",
    "\n",
    "Let's refer to $P(|X-x_0|< Δ(x)$ as $P(X \\sim x)$.\n",
    "\n",
    "So, for continuous dataset:\n",
    "### $P(Y\\sim y|X\\sim x) \\propto P(Y=y) \\cdot \\prod_{i=1}^{n} P(X\\sim x_i|Y\\sim y)$\n",
    "\n",
    "\n",
    "Therefore, an alternative option for writing out probabilities proportion equation is:\n",
    "### $PDF(y|x)Δ(y) \\propto P(y) \\cdot \\prod_{i=1}^{n} PDF(x_i|y)Δ(x)$\n",
    "\n",
    "Dividing by $Δ(y), Δ(x_i)$ will keep the proportion (they are positive constants):\n",
    "### $P(y|x) \\propto PDF(y|x) \\propto P(y) \\cdot \\prod_{i=1}^{n} PDF(x_i|y)$\n",
    "Summing up, we can calculate the relations between the __densities__ instead of the actual conditional probablities to understand which class is most likely for a specific data point.\n",
    "In order to use this method, we will need to know the distribution of the data. We can either assume a specific distribution, or try to approximate it.\n",
    "\n",
    "## Kernel Density Estimation\n",
    "Kernel density estimation (KDE) is a non-parametric method for estimating the probability density function of a given random variable. Here's a wonderfull interactive explanation: https://mathisonian.github.io/kde/.\n",
    "This method takes independent samples which are identically distributed, $x_1, ... x_n$.\n",
    "Intuitively, the KDE takes a window that calls bandwidth (similar to standard deviation) and runs it through the dataset.\n",
    "The more datapoints included inside the bandwidth, means greater density and results in higher value for the KDE.\n",
    "The KDE values are calculated by a weighted average of the distance between any point in the window, to a given point x, then divided by the bandwidth (hence the density). the weights come from the hyperparameter 'kernel', which is a non-negative function (https://en.wikipedia.org/wiki/Kernel_density_estimation#Definition). Mathematically:\n",
    "### $\\frac{1}{n\\cdot h} \\cdot \\sum_{i=1}^{n} K(\\frac{x-x_i}{h})$\n",
    "where K is the kernel function and h is the bandwidth.\n",
    "\n",
    "## Algorithm:\n",
    "### First part: Naive Bayes\n",
    "- __Assuming normal distribution__ for all the dataset.\n",
    "- Calculate Priors using the relative frequency of each class in the dataset.\n",
    "- Calculate the density function's values for each data point being in each class.\n",
    "- Calculate score (which is the product of prior and PDF values) for each data point being in each class\n",
    "- Among all of the calculated values, assign each of the data points to the class that has the highest score.\n",
    "\n",
    "### Second part: Not so Naive Bayes\n",
    "- Calculate Priors using the relative frequency of each class in the dataset.\n",
    "- For each feature and each class, fit the KDE with the data (4 features * 3 classes = 12 instances).\n",
    "- Calculate score (which is the product of prior and PDF values) for each data point being in each class\n",
    "- Among all of the calculated values, assign each of the data points to the class that has the highest score.\n",
    "\n",
    "### Third part: Best possible hyperparameters\n",
    "- Perform gridsearch for the two KDE hyperparameters: kernel and bandwith.\n",
    "- calculate the mean of cross-validation score for each hyperparameters pairing.\n",
    "- Get the best hyperparameters for the dataset and create new estimator with them.\n",
    "- Predict using the new estimator and compare the result to the Naive Bayes method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = np.c_[iris.data, iris.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByClass(dataset):\n",
    "    split = {}\n",
    "    for idx, class_num in enumerate(y):\n",
    "        if class_num not in split:\n",
    "            split[class_num] = dataset[idx]\n",
    "        else:\n",
    "            split[class_num] = np.vstack((split[class_num], dataset[idx]))\n",
    "    return split\n",
    "\n",
    "split = splitByClass(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitPredict(npDist, data, row):\n",
    "    params = npDist.fit(data)\n",
    "    arg = params[:-2]\n",
    "    mean = params[-2]\n",
    "    std = params[-1]\n",
    "    pdf = npDist.pdf(X[row], loc=mean, scale=std, *arg)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the score calculation, instead of calculating the product of probabilities we'll calculate the sum of logs. That is to avoid getting very small numerical values, which can result in numerical instability. Taking a log of that product (very small positive number) results in a much larger negative value, thus avoiding the problem. Also, the log of product equals to the sum of logs so we can calculate the following score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(prior, dist0, dist1, dist2, dist3):\n",
    "    return np.log(prior) +  np.log(dist0) +  np.log(dist1) +  np.log(dist2) +  np.log(dist3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate prior probabilities P(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_setosa = len(X[y==0]) / len(X)\n",
    "prior_versicolor = len(X[y==1]) / len(X)\n",
    "prior_virginica = len(X[y==2]) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normdist = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalScores(dist, data, splitted):\n",
    "    Y_pred = []\n",
    "    for i in range(len(data)):\n",
    "        d = {0: score(prior_setosa, fitPredict(dist, splitted[0][:, 0], i)[0],\n",
    "            fitPredict(dist, splitted[0][:, 1], i)[1],\n",
    "            fitPredict(dist, splitted[0][:, 2], i)[2],\n",
    "            fitPredict(dist, splitted[0][:, 3], i)[3]),\n",
    "             1: score(prior_versicolor, fitPredict(dist, splitted[1][:, 0], i)[0],\n",
    "            fitPredict(dist, splitted[1][:, 1], i)[1],\n",
    "            fitPredict(dist, splitted[1][:, 2], i)[2],\n",
    "            fitPredict(dist, splitted[1][:, 3], i)[3]), \n",
    "              2: score(prior_virginica, fitPredict(dist, splitted[2][:, 0], i)[0],\n",
    "            fitPredict(dist, splitted[2][:, 1], i)[1],\n",
    "            fitPredict(dist, splitted[2][:, 2], i)[2],\n",
    "            fitPredict(dist, splitted[2][:, 3], i)[3])}\n",
    "        madataVal = max(d, key = lambda k: d[k])\n",
    "        Y_pred.append(madataVal)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predNorm = totalScores(normdist, X, split)\n",
    "norm_cm = confusion_matrix(y, y_predNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 47,  3],\n",
       "       [ 0,  3, 47]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: Not so Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDENaiveBayes(BaseEstimator):\n",
    "    \"\"\"A version of Naive Bayes that uses KDE to estimate the \n",
    "    density distribution  for every combination of feature and class,\n",
    "    instead of assuming single distribution for all of the dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel = 'gaussian', bandwidth = 1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        self.splitByClass_ = [X[y == yi] for yi in self.classes_]\n",
    "        self.KDEs_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "                                   kernel=self.kernel).fit(x) for x in self.splitByClass_]\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.logPriors_ = [np.log(np.size(x[0], axis=0) / np.size(X[0], axis=0)) for x in self.splitByClass_]\n",
    "        logScore = np.array([kde.score_samples(X) for kde in self.KDEs_]).T\n",
    "        inverse = np.exp(logScore + self.logPriors_)\n",
    "        return inverse / inverse.sum(1, keepdims=True)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  0,  0],\n",
       "       [ 0, 17,  1],\n",
       "       [ 0,  2,  9]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kde = KDENaiveBayes()\n",
    "kde.fit(X_train,y_train)\n",
    "y_pred = kde.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KDENaiveBayes(), X, y, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's quickly compare to scikit-learn's Naive Bayes performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "cross_val_score(GaussianNB(), X, y, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're outscored :( Let's see if we can get a better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third part: Best possible hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll add a method named 'GridFitParams' to our class, in order to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDENaiveBayes(BaseEstimator):\n",
    "    \"\"\"A version of Naive Bayes that uses KDE to estimate the \n",
    "    density distribution  for every combination of feature and class,\n",
    "    instead of assuming single distribution for all of the dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel = 'gaussian', bandwidth = 0.5):\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "        \n",
    "    def GridFitParams(self, X, y):\n",
    "        bandwidths = np.arange(start=0.05, stop=2.0, step=0.05)\n",
    "        grid = GridSearchCV(KDENaiveBayes(), {'bandwidth': bandwidths,\n",
    "                                      'kernel': ['gaussian', 'tophat', 'epanechnikov',\n",
    "                                                 'exponential', 'linear', 'cosine']}, scoring='accuracy')\n",
    "        grid.fit(X, y)\n",
    "        print(\"best parameters: \", grid.best_params_)\n",
    "        return grid.best_params_\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        self.splitByClass_ = [X[y == yi] for yi in self.classes_]\n",
    "        self.KDEs_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "                                   kernel=self.kernel).fit(x) for x in self.splitByClass_]\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.logPriors_ = [np.log(np.size(x[0], axis=0) / np.size(X[0], axis=0)) for x in self.splitByClass_]\n",
    "        logScore = np.array([kde.score_samples(X) for kde in self.KDEs_]).T\n",
    "        inverse = np.exp(logScore + self.logPriors_)\n",
    "        return inverse / inverse.sum(1, keepdims=True)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'bandwidth': 1.05, 'kernel': 'tophat'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bandwidth': 1.05, 'kernel': 'tophat'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedKDE = KDENaiveBayes()\n",
    "best_params = fittedKDE.GridFitParams(X, y)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9600000000000002"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KDENaiveBayes(kernel=best_params['kernel'], bandwidth=best_params['bandwidth']),\n",
    "                X, y, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a win!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
